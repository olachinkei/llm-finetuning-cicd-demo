{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be429804-252e-4e69-b5bb-15fc8160e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lm-eval==0.3.0 -qqq\n",
    "#!git clone https://github.com/EleutherAI/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae36e6b-bf2c-4c62-a414-74d261a1f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ./lm-evaluation-harness/. -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0fc07c2-ad0b-4200-a7cb-b0a6e65d8c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Thanks for trying out the Report API!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For a tutorial, check out https://colab.research.google.com/drive/1CzyJx1nuOS4pdkXa2XPaRQyZdmFmLmXV\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Try out tab completion to see what's available.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ‚àü everything:    `wr.<tab>`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       ‚àü panels:    `wr.panels.<tab>`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       ‚àü blocks:    `wr.blocks.<tab>`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       ‚àü helpers:   `wr.helpers.<tab>`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       ‚àü templates: `wr.templates.<tab>`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For bugs/feature requests, please create an issue on github: https://github.com/wandb/wandb/issues\n"
     ]
    }
   ],
   "source": [
    "import ctranslate2\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import peft\n",
    "import random\n",
    "import timeit\n",
    "import urllib\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from lm_eval import tasks, evaluator, utils\n",
    "import lm_eval\n",
    "import wandb\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft import PeftModel, PeftConfig\n",
    "import wandb.apis.reports as wr\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6174c8-3d3e-41fb-a633-bcff7b132d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"Autocompletion with evaluation\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"reviewco\"\n",
    "os.environ[\"WANDB_USERNAME\"] = \"keisuke-kamata\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
    "\n",
    "SCORE_TABLE_NAME = \"Score\"\n",
    "EVALUATION_TABLE_NAME = \"Validation Responses\"\n",
    "LATENCY_TABLE_NAME = \"Model Latencies\"\n",
    "MODEL_NAME = \"Finetuned-Review-Autocompletion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e4c36-b273-4daf-8f96-22f92fb3bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(job_type=\"evaluation\")\n",
    "artifact = run.use_artifact('reviewco/Autocompletion with evaluation/finetuned-model:v1', type='model')\n",
    "artifact_dir = artifact.download()    \n",
    "base_llm = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", local_files_only=True)\n",
    "model = PeftModel.from_pretrained(base_llm, artifact_dir,torch_dtype=torch.float16)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e855b08-68c9-4cff-b7db-ae91577b54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeisuke-kamata\u001b[0m (\u001b[33mreviewco\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/wandb/run-20230828_003215-dkib959q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/runs/dkib959q' target=\"_blank\">scarlet-pond-32</a></strong> to <a href='https://wandb.ai/reviewco/Autocompletion%20with%20evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reviewco/Autocompletion%20with%20evaluation' target=\"_blank\">https://wandb.ai/reviewco/Autocompletion%20with%20evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/runs/dkib959q' target=\"_blank\">https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/runs/dkib959q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-pond-32</strong> at: <a href='https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/runs/dkib959q' target=\"_blank\">https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/runs/dkib959q</a><br/> View job at <a href='https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzMTA4NTQ0/version_details/v1' target=\"_blank\">https://wandb.ai/reviewco/Autocompletion%20with%20evaluation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkzMTA4NTQ0/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230828_003215-dkib959q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ë©ï‰æ°„ÇíÂÆüË°å\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mlm_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marc_easy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhellaswag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquad2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fewshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lm_eval/utils.py:161\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lm_eval/evaluator.py:68\u001b[0m, in \u001b[0;36msimple_evaluate\u001b[0;34m(model, model_args, tasks, num_fewshot, batch_size, device, no_cache, limit, bootstrap_iters, description_dict, check_integrity, decontamination_ngrams_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m     lm \u001b[38;5;241m=\u001b[39m lm_eval\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mget_model(model)\u001b[38;5;241m.\u001b[39mcreate_from_arg_string(\n\u001b[1;32m     65\u001b[0m         model_args, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device}\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, lm_eval\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mLM)\n\u001b[1;32m     69\u001b[0m     lm \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_cache:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ë©ï‰æ°„ÇíÂÆüË°å\n",
    "results = lm_eval.evaluator.simple_evaluate(\n",
    "    model=model,\n",
    "    tasks=[\"arc_easy\",\"hellaswag\",\"squad2\"],  \n",
    "    batch_size=16,\n",
    "    num_fewshot=3,\n",
    "    device=\"cuda\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece5e21-e6de-4918-bbca-de37c1359aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_responses_batch(prompts, ft_path):\n",
    "    # Get completions for each model in batches\n",
    "    opt_completions = get_huggingface_completion_batch(prompts, \"facebook/opt-125m\")\n",
    "    ft_completions = get_huggingface_completion_batch(prompts, ft_path)\n",
    "    responses = []\n",
    "    for opt, ft in zip(opt_completions, ft_completions):\n",
    "        responses.append({\n",
    "            \"Production\": opt,\n",
    "            \"Staging (finetuned)\": ft\n",
    "        })\n",
    "    return responses\n",
    "\n",
    "def get_huggingface_completion_batch(prompts, model):\n",
    "    generator = pipeline('text-generation', model=model)\n",
    "    responses = generator(prompts, max_new_tokens=50)\n",
    "    completions = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        full_output = responses[i][0][\"generated_text\"]\n",
    "        output = full_output[len(prompt):] if full_output.startswith(prompt) else full_output\n",
    "        completions.append(output.strip())\n",
    "    return completions\n",
    "\n",
    "\n",
    "def get_model_comparison_df(prompts, ft_path):\n",
    "    trimmed_prompts = [\n",
    "        \" \".join(prompt.split()[:random.randint(5,12)])\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    responses = get_completion_responses_batch(trimmed_prompts, ft_path)\n",
    "    df = pd.DataFrame(responses)\n",
    "    df.insert(0, \"prompt\", trimmed_prompts)\n",
    "    return df\n",
    "\n",
    "def get_latency_df(prompts, num_prompts, ft_path, ct2_path):\n",
    "  prompts = prompts.to_list()[:num_prompts]\n",
    "  ft_time = timeit.timeit(lambda: get_huggingface_completion_batch(prompts, model), number=1)\n",
    "  opt_time = timeit.timeit(lambda: get_huggingface_completion_batch(prompts, \"facebook/opt-125m\"), number=1)\n",
    "\n",
    "  return pd.DataFrame({\n",
    "    \"Model\": [\"Production\", \"Staging (finetuned)\"],\n",
    "    f\"Latency for {num_prompts} Reviews\": [opt_time, ft_time],\n",
    "  })\n",
    "\n",
    "def llm_evaluation_harnes_get_result(model_path, task_names):\n",
    "    base_llm = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", local_files_only=True)\n",
    "    model = PeftModel.from_pretrained(base_llm, model_path, torch_dtype=torch.float16)\n",
    "    # model.half()\n",
    "    \n",
    "    results = []\n",
    "    for task_name in task_names:\n",
    "        task = tasks.get_task(task_name)\n",
    "        metric = tasks.evaluate(model, tokenizer)\n",
    "        results.append((task_name, metric))\n",
    "    return results\n",
    "    \n",
    "def llm_evaluation_harnes_to_table(model_path):\n",
    "    task_names = ['hellaswag', 'ethics_justice','mathqa','squad2','wmt20-en-ja']  \n",
    "    results = llm_evaluation_harnes_get_result(model_path, task_names)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27eb22-ecf4-4f74-bd96-c7a96f0e9ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6189a-6d8a-49a9-b0a3-044c31fb2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd82c42-810b-4f6f-b48c-446074032143",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378317e0-a89d-44bd-b7c3-1e3799746437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(job_type=\"model_evaluation\") as run:\n",
    "    staging_model = wandb.use_artifact(f'{os.environ[\"WANDB_ENTITY\"]}/model-registry/{MODEL_NAME}:staging')\n",
    "    staging_path = staging_model.download()\n",
    "\n",
    "    reviews_artifact = run.use_artifact(f'{os.environ[\"WANDB_ENTITY\"]}/{os.environ[\"WANDB_PROJECT\"]}/reviews:production')\n",
    "    reviews_dir = reviews_artifact.download()\n",
    "\n",
    "    test_files = glob.glob(f\"{reviews_dir}/test/*.parquet\")\n",
    "    test_data = pd.concat([pd.read_parquet(path) for path in test_files])\n",
    "    prompts = test_data.sample(frac=1)[\"text\"][:10]\n",
    "\n",
    "    wandb.log({\n",
    "        EVALUATION_TABLE_NAME: get_model_comparison_df(prompts, ft_path=staging_path),\n",
    "        LATENCY_TABLE_NAME: get_latency_df(prompts, num_prompts=3, ft_path=staging_path)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68ebe4-4603-4b9b-864f-1ab78b6481ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Report\n",
    "    report = wr.Report(\n",
    "        project=os.environ[\"WANDB_PROJECT\"],\n",
    "        entity=os.environ[\"WANDB_ENTITY\"],\n",
    "        title='Model Evaluation: Autocompletion Model',\n",
    "        description=\"Data and sample predictions to evaluate the staging candidate model for our review autocompletion algorithm.\"\n",
    "    )\n",
    "\n",
    "    report.width = \"fluid\"\n",
    "\n",
    "    runsets = [wr.Runset(\n",
    "        os.environ[\"WANDB_ENTITY\"],\n",
    "        os.environ['WANDB_PROJECT']\n",
    "        )]\n",
    "\n",
    "    report.blocks = [\n",
    "        wr.TableOfContents(),\n",
    "        wr.H1(\"Report Overview\"),\n",
    "        wr.P(\n",
    "            \"This report contains information to evaluate whether potential staging models \"\n",
    "            \"should be moved to production. Model Registry admins can use the view of the \"\n",
    "            \"Model Registry at the end of this report to move a staging model into production, \"\n",
    "            \"using a Webhook automation.\"\n",
    "        ),\n",
    "        wr.Spotify(spotify_id=\"7KAveXwQ5xzdHT6GDlNIBu\"),\n",
    "        wr.MarkdownBlock(\"May this staging model earn 5 stars üôè.\"),\n",
    "        wr.HorizontalRule(),\n",
    "    ]\n",
    "\n",
    "    pg = wr.PanelGrid(\n",
    "        runsets=runsets,\n",
    "        panels=[\n",
    "        wr.ScalarChart(\n",
    "            title=\"Current Min Eval Loss\",\n",
    "            metric=\"eval/loss\",\n",
    "            groupby_aggfunc=\"min\",\n",
    "            font_size=\"large\"),\n",
    "\n",
    "        wr.ScalarChart(\n",
    "            title=\"Current Min Train Loss\",\n",
    "            metric=\"train/loss\",\n",
    "            groupby_aggfunc=\"min\",\n",
    "            font_size=\"large\"),\n",
    "\n",
    "        wr.ScalarChart(\n",
    "            title=\"Longest Runtime (sec)\",\n",
    "            metric=\"train/train_runtime\",\n",
    "            groupby_aggfunc=\"max\",\n",
    "            font_size=\"large\"),\n",
    "\n",
    "        wr.LinePlot(x='Step',\n",
    "                    y=['eval/loss'],\n",
    "                    smoothing_factor=0.8,\n",
    "                    layout={'w': 24, 'h': 9})\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    report.blocks += [wr.H1(\"Key Metrics\"), pg]\n",
    "\n",
    "    pg = wr.PanelGrid(\n",
    "        runsets=runsets,\n",
    "        panels=[\n",
    "            wr.WeavePanelSummaryTable(LATENCY_TABLE_NAME, layout={'w': 24, 'h': 12}),\n",
    "        ])\n",
    "\n",
    "    report.blocks += [wr.H1(\"Latency Data for Models\"), pg]\n",
    "\n",
    "\n",
    "    pg = wr.PanelGrid(\n",
    "        runsets=runsets,\n",
    "        panels=[\n",
    "            wr.WeavePanelSummaryTable(EVALUATION_TABLE_NAME, layout={'w': 24, 'h': 12}),\n",
    "        ])\n",
    "\n",
    "    report.blocks += [wr.H1(\"Sample Predictions\"), pg]\n",
    "\n",
    "    report.blocks += [wr.H1(\"Autocompletion Model in Model Registry\"), wr.WeaveBlockArtifact(os.environ[\"WANDB_ENTITY\"], \"model-registry\", MODEL_NAME)]\n",
    "    report.save()\n",
    "\n",
    "    report_creation_msg = f\"Report to review: {urllib.parse.quote(report.url, safe=r'/:')}\"\n",
    "    print(report_creation_msg)\n",
    "\n",
    "    wandb.alert(\"New Staging Model Evaluated\", report_creation_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
