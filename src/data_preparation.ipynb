{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafda116-e024-4753-b676-399b87d464c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc0207-7307-4dc0-bd9d-095726150c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_ENTITY\"] = \"reviewco\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Autocompletion with evaluation\"\n",
    "os.environ[\"WANDB_USERNAME\"] = \"keisuke-kamata\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235adb8-5f46-43f8-bb8d-3d262110b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1059db7-62b9-44c2-b75f-8df9a3398d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"databricks/databricks-dolly-15k\",split=\"train\")\n",
    "data_train_test = data.train_test_split(test_size=0.2)\n",
    "data_train_valid = data_train_test[\"train\"]\n",
    "data_train_valid = data_train_valid.train_test_split(test_size=0.2)\n",
    "data_train = data_train_valid[\"train\"]\n",
    "data_valid = data_train_valid[\"test\"]\n",
    "data_test = data_train_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948284b9-61ae-42bd-ae77-3463cfd84f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response\"\"\"\n",
    "\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "Input:\n",
    "{context}\n",
    "### Response\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65d80d-0439-4f09-a354-6f3818aba5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, json_list, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = ignore_index\n",
    "        self.features = []\n",
    "        \n",
    "        for j in tqdm(json_list):\n",
    "            # In cases like open_qa where context information is not necessary, there is no input column.\n",
    "            # Therefore, we differentiate the template sentences based on whether the input column is present or not.\n",
    "            if 'context' in j:\n",
    "                source_text = PROMPT_WITH_INPUT_FORMAT.format_map(j)\n",
    "            else:\n",
    "                source_text = PROMPT_NO_INPUT_FORMAT.format_map(j)\n",
    "            \n",
    "            # Combine the instruction sentence and the response sentence, and insert an EOS token at the end\n",
    "            example_text = source_text + j['response'] + self.tokenizer.eos_token\n",
    "            \n",
    "            # okenize only the instruction sentence (up to 'The following is a task to ~### Response:')\n",
    "            # What we want is the length of the instruction sentence.\n",
    "            source_tokenized = self.tokenizer(\n",
    "                source_text,\n",
    "                padding='longest',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_length=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Tokenize both the instruction sentence and the response sentence\n",
    "            example_tokenized = self.tokenizer(\n",
    "                example_text, \n",
    "                padding='longest', \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = example_tokenized['input_ids'][0]\n",
    "            \n",
    "            # Copy the input sentence as is to be the correct answer that the LLM generates.\n",
    "            labels = copy.deepcopy(input_ids)\n",
    "            \n",
    "            # Length up to the instruction sentence\n",
    "            source_len = source_tokenized['length'][0]\n",
    "            \n",
    "            # Since the desired correct sentence for the LLM to generate also includes the instruction sentence,\n",
    "            # we fill the section of the instruction sentence with -100 as IGNORE_INDEX to avoid calculating the CrossEntropyLoss.\n",
    "            labels[:source_len] = self.ignore_index\n",
    "            \n",
    "            self.features.append({\n",
    "                'input_ids': input_ids,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "        \n",
    "class InstructCollator():\n",
    "    def __init__(self, tokenizer, ignore_index=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        input_batch = []\n",
    "        label_batch = []\n",
    "        for example in examples:\n",
    "            input_batch.append(example['input_ids'])\n",
    "            label_batch.append(example['labels'])\n",
    "        \n",
    "        input_ids = pad_sequence(\n",
    "            input_batch, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        # labelsのpaddingトークンは先程と同様にignore_indexである-100で埋める\n",
    "        labels = pad_sequence(\n",
    "            label_batch, batch_first=True, padding_value=self.ignore_index\n",
    "        )\n",
    "\n",
    "        # attention_maskはbool値でもいいらしい\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
